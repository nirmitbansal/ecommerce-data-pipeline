**ğŸ›ï¸ E-commerce Data Pipeline**  
  ğŸ“¦ End-to-End ETL, Warehouse & Analytics Workflow

This project is a complete end-to-end ETL pipeline built for beginners who want hands-on experience in data engineering, using only Python, Pandas, and SQLite.  
It takes raw e-commerce order data (CSV), cleans and transforms it, loads it into a database, and runs analytical SQL queries.

**This project demonstrates:**  
    ğŸ—‚ï¸ Data extraction from CSV  
    ğŸ§¹ Data cleaning & transformation using Pandas  
    ğŸ—„ï¸ Loading data into a SQLite database  
    ğŸ“Š Running SQL-based analytics  
    ğŸ§± Modular project structure  
    ğŸ”„ How to run ETL pipelines locally

**Data Pipeline Workflow**

âœ Extraction   
      â€¢Reads raw order data from CSV files in the data/ folder.

âœTransformation   
      â€¢Cleans nulls and inconsistent values  
      â€¢Standardizes formats  
      â€¢Derives additional fields where needed
    
âœLoading   
      â€¢Writes cleaned tables into a SQLite database file  
      â€¢Ensures tables are indexed for performance

**ğŸ—ï¸ Architecture**  
ğŸ“¥ Raw E-commerce Event Files  
â†“  
ğŸ› ï¸ Chunked Data Cleansing  
â†“  
ğŸ“Œ Transformation & Feature Creation  
â†“  
ğŸ—ƒï¸ Parquet Staging Storage  
â†“  
âš¡ DuckDB Warehouse Views  
â†“  
ğŸ“Š Analytics & KPI Outputs  

ğŸ§  What Makes This Pipeline Great   
      âœ”ï¸ Process **large datasets** without memory issues  
      âœ”ï¸ Maintain **data quality** with cleansing rules  
      âœ”ï¸ Store efficient Parquet files for reuse  
      âœ”ï¸ Use DuckDB for **fast SQL analytics**  
      âœ”ï¸ Output KPI tables for business reporting

**ğŸ“ Repository Structure**
```text
â”£ ğŸ“‚ src/ # ETL + warehouse + analytics scripts
â”£ ğŸ“„ requirements.txt # Python dependencies
â”— ğŸ“„ README.md # This file
